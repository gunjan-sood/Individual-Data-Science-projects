{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import matplotlib.pyplot\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "\n",
    "class Wikicrawler():\n",
    "  \"\"\"\n",
    "    Gone are the days when All roads leaded to Rome, now every link leads to Philosophy wiki page.\n",
    "\n",
    "      Quoting wikipedia-\"As of Feb ,2016, 97% of all articles in Wikipedia lead eventually to the article Philosophy.\"\n",
    "      Link : http://en.wikipedia.org/wiki/Wikipedia:Getting_to_Philosophy\n",
    "\n",
    "    This crawler, does exactly what it intends to : Crawls Wikipedia starting from  random page and tracks the click\n",
    "    path to the Philosophy Wikipedia page. This process is repeated 10 times\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.base_url = \"https://en.wikipedia.org\"\n",
    "    self.philosophy_page = \"https://en.wikipedia.org/wiki/Philosophy\"\n",
    "    self.wikilink = r'(?<=<a href=\")/wiki/[a-zA-Z\\(\\)\\-\\,_#]*?(?=\")'  # Regular expression for href to the next link\n",
    "    self.visited_urls = {} # Store urls as keys and path length as values\n",
    "    self.not_unavailable = {} #Store urls that do not reach Philosphy page. Value = -1\n",
    "    self.counter = 0 #Path length from starting random page\n",
    "    self.paths_tested = 0 #Counter for total number of pages tried.\n",
    "    self.path_length_dict = {} #For distribution plots. Key = path length: Value = Frequency\n",
    "\n",
    "\n",
    "  def crawler(self, start_page):\n",
    "    \"\"\"\n",
    "      Returns:  1 on success of entering a url entry to one of the two dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    route_length = self.philosophy_test(start_page)\n",
    "\n",
    "    if route_length >= 0: #In case of successful path to Philosophy page\n",
    "      self.visited_urls[start_page] = route_length\n",
    "      print \"It took %i\" % route_length +\" \"+\"links to reach to Philosphy page\"\n",
    "      return 1\n",
    "    elif route_length == -1: #In case of failure to reach Philosophy page\n",
    "      self.not_unavailable[start_page] = -1\n",
    "      print \"No path found to philosophy page\"\n",
    "      return 1\n",
    "    else:\n",
    "      print(\"error line 38\") #error message should not be reached\n",
    "\n",
    "\n",
    "  def philosophy_test(self, url):\n",
    "    \"\"\"\n",
    "      Checks to see relation to Philosophy page\n",
    "      Args: url - string of url being assessed\n",
    "      Returns: counter or -1 if no possible relation to philosophy page\n",
    "    \"\"\"\n",
    "    if url == -100: #in case of no path to Philosophy page\n",
    "      return -1\n",
    "\n",
    "    if url == self.philosophy_page: # in case of reaching Philosophy page\n",
    "      return self.counter\n",
    "\n",
    "    elif url in self.visited_urls: #in case of reaching a page already stored in visited_urls dictionary\n",
    "      counter = self.counter + self.visited_urls[url]\n",
    "      return counter\n",
    "\n",
    "    elif self.counter >= 45: #limits path searches to 45 clicks to limit possible infinite loops\n",
    "      return -1\n",
    "\n",
    "    elif url != self.philosophy_page: #recursively call function to keep going until path is resolved\n",
    "      self.counter += 1\n",
    "      next_url = self.next_link(url)\n",
    "      test = self.philosophy_test(next_url)\n",
    "      return test\n",
    "    else:\n",
    "      return -100 #should not hit this error\n",
    "\n",
    "\n",
    "  def next_link(self, url):\n",
    "    \"\"\"\n",
    "      Args: url - url to be parsed as a string\n",
    "      Returns: the next page to be assessed or a kick out if no next page\n",
    "    \"\"\"\n",
    "    r = requests.get(url)\n",
    "    page = r.text\n",
    "    soup = BeautifulSoup(page, 'html.parser') #inputs the text from the page and parses it\n",
    "    body = soup.select('div#mw-content-text > p') #body becomes all p elements in the content div\n",
    "    if body == None: #check to make sure there is a body\n",
    "      return -100\n",
    "    cleaned_body = self.remove_parenthesis(str(body)) #take out strings between parentheses\n",
    "    match = re.search(self.wikilink, cleaned_body) #checks the cleaned_body for next wiki page link using regex\n",
    "    if match:\n",
    "      href = match.group(0).split('#')[0] #next page link\n",
    "      if href == '': #if href is not a link\n",
    "        href = self.check_if_list(soup) #check to see href is in an li element\n",
    "      new_page = self.base_url + href\n",
    "      print \"Going to:\"+' '+new_page\n",
    "      return new_page #next page url\n",
    "    elif not match: #if no link return the kickout to say no path possible\n",
    "        return -100\n",
    "\n",
    "\n",
    "\n",
    "  def check_if_list(self, soup):\n",
    "    \"\"\"\n",
    "      Checks to see if link is in a li element if no links in p elements\n",
    "      Args: soup - parsed html\n",
    "      Returns: Link suffix if there is a link, else False\n",
    "    \"\"\"\n",
    "    body_list = soup.select('div#mw-content-text > ul > li')\n",
    "    cleaned_body = self.remove_parenthesis(str(body_list))\n",
    "    match = re.search(self.wikilink, cleaned_body)\n",
    "    if match != 0:\n",
    "      href = match.group(0).split('#')[0]\n",
    "      if href != None:\n",
    "        return href\n",
    "      else:\n",
    "        return False\n",
    "    else:\n",
    "      return False\n",
    "\n",
    "  def remove_parenthesis(self, text):\n",
    "    \"\"\"\n",
    "      Removes content between parenthesis from a string,\n",
    "      but leaves <tags> alone.\n",
    "      Args: string - the body must be turned into a string\n",
    "      Returns: The body of the text with content(s) between parentesis removed\n",
    "    \"\"\"\n",
    "    paren_counter = 0 # becomes 0 (closed) or 1 (open) to check if parenthesis is open or closed\n",
    "    tag_counter = 0 #becomes 0 or 1 to check if tag is open or closed\n",
    "    cleaned = ''\n",
    "    for i in text:\n",
    "      if i == '<':\n",
    "        tag_counter += 1\n",
    "      elif i == '>':\n",
    "        tag_counter -= 1\n",
    "      elif i == '(' and tag_counter == 0:\n",
    "        paren_counter += 1\n",
    "      elif i == ')' and tag_counter == 0:\n",
    "        paren_counter -= 1\n",
    "      if paren_counter == 0:\n",
    "        cleaned += i\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "#2 component - writing the result to CSV file\n",
    "  def write_csv(self):\n",
    "    \"\"\"\n",
    "      Turns the results into a csv file\n",
    "      Url is in one column, and path length is in another column.\n",
    "    \"\"\"\n",
    "    with open('data.csv', 'w') as csvfile:\n",
    "      fieldnames = ['url', 'clicks']\n",
    "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "      writer.writeheader()\n",
    "      for key, value in self.visited_urls.items():\n",
    "        writer.writerow({'url': key, 'clicks': value})\n",
    "      for key, value in self.unavailable.items():\n",
    "        writer.writerow({'url': key, 'clicks': value})\n",
    "\n",
    "\n",
    "#3 component - calculate the percentage of success and then plotting\n",
    "  def percent(self):\n",
    "    \"\"\"\n",
    "      Checks successful rate of linking to Philosophy page\n",
    "    \"\"\"\n",
    "    success = len(self.visited_urls) #checks length of visited_urls dic\n",
    "    print(\"success: \", success)\n",
    "    failed = len(self.not_unavailable) #checks length of unavailable dic\n",
    "    print(\"failed: \", failed)\n",
    "    total = success + failed\n",
    "    print total\n",
    "    percentage = (float(success)/total)*100 #creates percentage\n",
    "    return percentage\n",
    "\n",
    "  def distribution(self):\n",
    "    \"\"\"\n",
    "      Sets x and y axis and plots distribution using matplotlib for successful paths\n",
    "    \"\"\"\n",
    "    xaxis = [] #path lengths\n",
    "    yaxis = [] #frequency\n",
    "\n",
    "    for key, value in self.visited_urls.items():\n",
    "      if value not in self.path_length_dict: #adds new paths lengths to dictionary\n",
    "        self.path_length_dict[value] = 1\n",
    "      else: #adds frequency to dictionary\n",
    "        self.path_length_dict[value] += 1\n",
    "\n",
    "    for key in self.path_length_dict.keys(): #adds to the x and y axis lists\n",
    "      xaxis.append(int(key))\n",
    "      yaxis.append(int(self.path_length_dict[key]))\n",
    "\n",
    "    #Plots data and creates a png file named 'plot'\n",
    "    matplotlib.pyplot.bar(xaxis, yaxis, align='center')\n",
    "    matplotlib.pyplot.xlabel('Path Length')\n",
    "    matplotlib.pyplot.ylabel('Frequency')\n",
    "    matplotlib.pyplot.title('Path length counts to Wikipedia Philosphy Page')\n",
    "    matplotlib.pyplot.savefig('plot.png')\n",
    "\n",
    "  def run(self):\n",
    "    \"\"\"\n",
    "      Run this function to run program.\n",
    "      Returns Success Rate\n",
    "    \"\"\"\n",
    "    self.counter = 0  # resets counter for each starting page\n",
    "    random_url = self.base_url + \"/wiki/Special:Random\"\n",
    "    start_page = requests.get(random_url)\n",
    "    start_page_url = start_page.url  # gets the landing url\n",
    "    path_tested = self.crawler(start_page_url)  # starts the crawling and returns 1 once finished\n",
    "\n",
    "    self.paths_tested = self.paths_tested + path_tested  # adds 1 to paths_tested for each completion of path\n",
    "    print \"%i\" % self.paths_tested+\" \"+\"path tested\"  # Shows progress in terminal\n",
    "    if self.paths_tested >= 20:  # stops paths tested at 10\n",
    "      percentage = self.percent()  # on completion check Percent of successful paths\n",
    "      print \"Percentage of success %i\" %percentage\n",
    "\n",
    "      distribution = self.distribution()  # on completion plot distribution\n",
    "      self.write_csv()  # on completion write data to a csv file\n",
    "\n",
    "    else:\n",
    "      self.run()\n",
    "\n",
    "\n",
    "pager = Wikicrawler()\n",
    "pager.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
